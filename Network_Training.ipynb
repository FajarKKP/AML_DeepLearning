{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6d1ciVelMBd1RVAhx+GB/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FajarKKP/Imperial_Related_Stuff/blob/main/Network_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coursework\n",
        "\n",
        "Task 1: Tuning a Classification Model\n",
        "In a machine learning problem, and especially when using a deep learning approach, finding the right set of hyperparameters, the right data augmentation strategy, or a good regularization method can make the difference between a model that performs poorly and a model with great accuracy.\n",
        "\n",
        "For this exercise, you will be training a CNN to perform classification in CIFAR-10 (we use the official test set, which is why the variables are called x_test and y_test, as our validation set) and will analyze the impact of some of the most important elements presented in this tutorial.\n",
        "\n",
        "Use the CNN we give in the code below, along with the given optimizer and number of training epochs as the default setting. Only modify the given CNN architecture to add Dropout or Batch Normalization layers when explicitly stated. Use 40 epochs to plot all of your curves. However, you can train for more epochs to find your best validation performance if your network has not finished training in those 40 epochs.\n",
        "\n",
        "Report\n",
        "\n",
        "*   First, train the given default model without any data augmentation. Then define two data augmentation strategies (one more aggressive than the other) and train the model with data augmentation. Clearly state the two augmentation strategies you apply (i.e., the specific transformations). Discuss the training and validation loss curves for the two data augmentation strategies along with the original run without data augmentation. Attach in the appendix those training and validation curves. Report in a table the best validation accuracy obtained for the three runs (no data augmentation, data augmentation 1, data augmentation 2).\n",
        "\n",
        "*   Without using any data augmentation, analyze the effect of using Dropout in the model. Carry out the same analysis for Batch Normalization. Finally, combine both. Report in the same table as in the data augmentation task the best validation accuracy for each of the three settings (baseline + Dropout, baseline + Batch Normalization, baseline + Batch Normalization + Dropout). The performance will vary depending on where the Dropout layers and Batch Normalization layers, so state clearly where you added the layers, and what rate you used for the Dropout layers. Discuss the results.\n",
        "\n",
        "*   Using the default model/hyperparameters and no data augmentation, report the best validation accuracy when using zeros for the kernel initialization. Report the performance in the same table as in the dropout/batch normalization/data augmentation tasks. Discuss the results that you obtained.\n",
        "\n",
        "*   Using the default model and no data augmentation, change the optimizer to SGD and train it with learning rates of 3e-3, 1e-3 and 3e-4. Report in a figure the training and validation loss for the three learning rate values and discuss the figure.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Z7U8_aq-YTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(X_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(X_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(X_test.shape[0]))\n",
        "\n",
        "X_train = X_train.reshape(-1,32,32,3)\n",
        "\n",
        "## Normalization block\n",
        "norm_layer = keras.layers.Normalization()\n",
        "norm_layer.adapt(X_train)\n",
        "X_train_n = norm_layer(X_train)\n",
        "X_test_n = norm_layer(X_test)\n",
        "\n",
        "# You can modify the data_augmentation variable below to add your\n",
        "# data augmentation pipeline.\n",
        "# By default we do not apply any augmentation (RandomZoom(0) is equivalent\n",
        "# to not performing any augmentation)\n",
        "data_augmentation = keras.models.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomZoom(0)\n",
        "    ]\n",
        ")\n",
        "# We will use glorot_uniform as a initialization by default\n",
        "initialization = 'glorot_uniform'\n",
        "# Use the architecture given below, only modify it to add Dropout/BatchNorm\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Input(shape=(32,32,3)))\n",
        "model.add(data_augmentation)\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), padding='same', kernel_initializer=initialization))\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer=initialization))\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "model.add(keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer=initialization))\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "model.add(keras.layers.Conv2D(256, (3, 3), padding='same', kernel_initializer=initialization))\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "# As we use global average pooling, we don't need to use Flatten\n",
        "model.add(keras.layers.GlobalAveragePooling2D())\n",
        "model.add(keras.layers.Dense(10, kernel_initializer=initialization))\n",
        "model.add(keras.layers.Activation('softmax'))\n",
        "\n",
        "\n",
        "Y_train_class = keras.utils.to_categorical(y_train, 10)\n",
        "Y_test_class = keras.utils.to_categorical(y_test, 10)\n",
        "# By default use Adam with lr=3e-4. Change it to SGD when asked to\n",
        "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "# Use 40 epochs as default value to plot your curves\n",
        "history = model.fit(X_train_n, Y_train_class, epochs=40, validation_data=(X_test_n,Y_test_class))"
      ],
      "metadata": {
        "id": "w9Z2DL4h-yRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq-Y64yu-O4d"
      },
      "outputs": [],
      "source": []
    }
  ]
}